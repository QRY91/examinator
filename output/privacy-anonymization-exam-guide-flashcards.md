# Privacy Anonymization Exam Guide - Flashcards

- **the difference between privacy and confidentiality**: Privacy is an individual's right to control how their personal data is used, while confidentiality is the obligation to keep information secret from unauthorized parties.
- **personally identifiable information (PII)**: Any information that can be used to identify a specific individual, either directly (name, SSN) or indirectly (combination of attributes).
- **quasi-identifiers**: Attributes that alone don't uniquely identify someone, but when combined with other data sources can lead to re-identification (e.g., age, ZIP code, gender).
- **data anonymization**: The process of removing or altering personally identifiable information so that individuals cannot be identified from the dataset.
- **the main anonymization techniques**: 1) Blanking (removing data), 2) Hashing (one-way transformation), 3) Masking (partial obscuring), 4) Generalization (making data less specific), 5) Suppression (removing records).
- **pseudonymization**: Replacing identifying information with artificial identifiers (pseudonyms) while maintaining a separate mapping that can be reversed if needed.
- **k-anonymity**: A privacy preservation technique ensuring that each record in a dataset is indistinguishable from at least k-1 other records based on quasi-identifiers.
- **How do you achieve k-anonymity**: 1) Generalization (making values less specific, e.g., exact age â†’ age ranges), 2) Suppression (removing outlier records), 3) Combination of both techniques.
- **What does k=3 anonymity mean?**: Every combination of quasi-identifier values appears at least 3 times in the dataset, so any individual is indistinguishable from at least 2 others.
- **the limitations of k-anonymity**: 1) Homogeneity attack (if all k records have same sensitive value), 2) Background knowledge attack, 3) Data utility loss through generalization.
- **l-diversity**: An extension of k-anonymity that ensures each equivalence class has at least l different values for sensitive attributes, preventing homogeneity attacks.
- **t-closeness**: Further refinement requiring that the distribution of sensitive attributes in each equivalence class is close to the overall distribution in the dataset.
- **differential privacy**: A privacy technique that adds carefully calibrated random noise to query results, providing mathematical guarantees about privacy while preserving statistical utility.
- **the advantages of differential privacy over k-anonymity**: 1) Mathematical privacy guarantees, 2) Composability (multiple queries), 3) Resistance to auxiliary information attacks, 4) Better utility preservation.
- **a linking attack**: An attack where an adversary combines the anonymized dataset with external data sources to re-identify individuals.
- **a homogeneity attack on k-anonymous data**: When all records in an equivalence class have the same sensitive attribute value, allowing inference even without exact identification.
- **What was the Netflix Prize dataset attack?**: A famous de-anonymization attack where researchers linked "anonymous" Netflix ratings with public IMDb ratings to identify users.
- **What factors should you consider when choosing k for k-anonymity?**: 1) Sensitivity of data, 2) Risk tolerance, 3) Data utility requirements, 4) Size of dataset, 5) Regulatory requirements.
- **How do you balance privacy and data utility**: 1) Choose appropriate k value, 2) Use domain hierarchies for generalization, 3) Consider which attributes to generalize, 4) Evaluate information loss metrics.
- **the difference between anonymization and de-identification**: De-identification removes direct identifiers but may still allow re-identification through quasi-identifiers. True anonymization makes re-identification practically impossible.